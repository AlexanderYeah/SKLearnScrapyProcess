# 爬虫前言介绍
###一 爬取网站前的准备操作
1 robots.txt  
 [百度解释](https://baike.baidu.com/item/robots%E5%8D%8F%E8%AE%AE/2483797?fr=aladdin&fromid=9518761&fromtitle=robots.txt)  
Robots协议（也称为爬虫协议、机器人协议等）的全称是“网络爬虫排除标准”（Robots Exclusion Protocol），网站通过Robots协议告诉搜索引擎哪些页面可以抓取，哪些页面不能抓取。  
示例robots.txt 文件  

UserAgent:BadCrawler  
Disallow:/
  
以上代码代表 禁止用户代理为BadCrawler 爬虫爬取网站  
User-agent:*  
Crawl-delay:5  
Disallow:/trap  
以上代码表示，无论是哪种用户代理，两次下载请求之间都应该间隔5秒钟，并且不允许访问/trap 链接，否则服务器封禁IP 一分钟。  

Sitemap:http://example.webscraping.com/sitemap.xml  
以上是定义了一个sitemap 文件  网站地图，提供了所有的网页链接。  

2 估算网站的大小  
3 识别网站所用的技术  
* 安装builtwith 模块  
> pip install builtwith    

* 将URL 作为参数，下载该URL 并且进行解析     

>import builtwith  

> builtwith.parse('http://example.webscraping.com')  

> {'web-servers': ['Nginx'], 'web-frameworks': ['Web2py', 'Twitter Bootstrap'], 'programming-languages': ['Python'], 'javascript-frameworks': ['jQuery', 'Modernizr', 'jQuery UI']}
   
 从上面的返回结果是 使用了服务器Nginx 框架等信息  
 
 4 寻找网站的所有者是谁  
 使用WHOIS协议查询域名的注册者是谁，python 中有一个针对该协议的封装库  
 
 * 安装  

> pip install python-whois  

* 打开python解释器  

> import  whois

> print whois.whois('baidu.com')    
 
就会返回baidu.com 的详细信息 

  
 


  





